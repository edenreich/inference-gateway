---
services:
  inference-gateway:
    image: ghcr.io/edenreich/inference-gateway:v0.1.3
    ports:
      - 8080:8080
    env_file:
      - .env

  ollama:
    image: ollama/ollama:latest
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "ollama serve & sleep 5 && ollama pull phi3:3.8b && tail -f /dev/null"
    environment:
      OLLAMA_HOST: 0.0.0.0:8080
    volumes:
      - docker-compose-ollama-data:/.ollama

volumes:
  docker-compose-ollama-data:
